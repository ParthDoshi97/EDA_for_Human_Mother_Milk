TRUE ~ "other"
))
attach(MIRIS.HM.Data.scale)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value", -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.scale)
Milk.Boxplot <- boxplot(MIRIS.HM.Data.scale)
# Assigning Class to Variable  according to information from sample ID
MIRIS.HM.Data.new <- MIRIS.HM.Data.scale %>%
mutate(Class = case_when(grepl("^POST", Samples) ~ "post",
grepl("^PRE", Samples) ~ "pre",
grepl("^P", Samples) ~ "PI",
grepl("^T", Samples) ~ "TI",
TRUE ~ "other"
))
attach(MIRIS.HM.Data.scale)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value", -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.scale)
print(Milk.Boxplot)
View(MIRIS.HM.Data.new)
View(MIRIS.HM.Data.scale)
View(MIRIS.HM.Data.new)
# Assigning Class to Variable  according to information from sample ID
MIRIS.HM.Data.new <- MIRIS.HM.Data.scale %>%
mutate(Class = case_when(grepl("^POST", Samples) ~ "post",
grepl("^PRE", Samples) ~ "pre",
grepl("^P", Samples) ~ "PI",
grepl("^T", Samples) ~ "TI",
TRUE ~ "other"
))
attach(MIRIS.HM.Data)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value",-Class, -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.new)
print(Milk.Boxplot)
# Calculate p-values for each correlation coefficient
p.mat <- cor_pmat(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Calculate correlation matrix using Pearson correlation
correlationMatrix <- cor(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Visualize the correlation matrix using ggcorrplot
ggcorrplot(
correlationMatrix,
hc.order = TRUE,   # Hierarchical clustering for reordering variables
type = "lower",    # Show only the lower triangle of the correlation matrix
lab = TRUE,        # Show labels for variables
p.mat = p.mat      # Overlay p-values on the plot
)
cor()
?cor()
# Calculate p-values for each correlation coefficient
p.mat <- cor_pmat(MIRIS.HM.Data.scale[2:7], method = "spearman")
# Calculate correlation matrix using Pearson correlation
correlationMatrix <- cor(MIRIS.HM.Data.scale[2:7], method = "spearman")
# Visualize the correlation matrix using ggcorrplot
ggcorrplot(
correlationMatrix,
hc.order = TRUE,   # Hierarchical clustering for reordering variables
type = "lower",    # Show only the lower triangle of the correlation matrix
lab = TRUE,        # Show labels for variables
p.mat = p.mat      # Overlay p-values on the plot
)
# Calculate p-values for each correlation coefficient
p.mat <- cor_pmat(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Calculate correlation matrix using Pearson correlation
correlationMatrix <- cor(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Visualize the correlation matrix using ggcorrplot
ggcorrplot(
correlationMatrix,
hc.order = TRUE,   # Hierarchical clustering for reordering variables
type = "lower",    # Show only the lower triangle of the correlation matrix
lab = TRUE,        # Show labels for variables
p.mat = p.mat      # Overlay p-values on the plot
)
# Calculate covariance matrix
cov_matrix <- cov(MIRIS.HM.Data.scale[2:7], method = "pearson")
cov_matrix <- cov2cor(cov_matrix)
cov_df_long <- melt(cov_matrix)
# Create the covariance plot using ggplot2
plot <- ggplot(cov_df_long, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient(high = "red", low = "white") +
geom_text(aes(label = paste0(round(value * 100, 2))), color = "black", size = 3) +
labs(title = "Covariance Heatmap",
x = "Variable 1",
y = "Variable 2")
print(plot)
# Perform Leven's test for each continuous variable
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Levene's test
var_test <- leveneTest(formula, data = MIRIS.HM.Data.new)
# Print the variable name and the test results
cat("Variable:", names(MIRIS.HM.Data.new)[i], "\n")
print(var_test)
cat("\n")
}
# Perform ANOVA and display summary for each model
for (vars in continuous.vars) {
formula <- as.formula(paste(vars, "~", categorical.var))
aov_model <- aov(formula, data = MIRIS.HM.Data.new)
tukey_test <- TukeyHSD(aov_model)
cat("Variable:", vars, "\n")
print(summary(aov_model))
print(tukey_test)
cat("\n")
}
# Perform Leven's test for each continuous variable
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Levene's test
var_test <- leveneTest(formula, data = MIRIS.HM.Data.new)
# Print the variable name and the test results
cat("Variable:", names(MIRIS.HM.Data.new)[i], "\n")
print(var_test)
cat("\n")
}
# Perform ANOVA and display summary for each model
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Anova
aov_model <- aov(formula, data = MIRIS.HM.Data.new)
# Perform Tukey Test
tukey_test <- TukeyHSD(aov_model)
cat("Variable:", vars, "\n")
# Print Results
print(summary(aov_model))
print(tukey_test)
cat("\n")
}
# Perform Leven's test for each continuous variable
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Levene's test
var_test <- leveneTest(formula, data = MIRIS.HM.Data.new)
# Print the variable name and the test results
cat("Variable:", names(MIRIS.HM.Data.new)[i], "\n")
print(var_test)
cat("\n")
}
# Perform ANOVA and display summary for each model
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Anova
aov_model <- aov(formula, data = MIRIS.HM.Data.new)
# Perform Tukey Test
tukey_test <- TukeyHSD(aov_model)
# Print Results
cat("Variable:", names(MIRIS.HM.Data.new)[i], "\n")
print(summary(aov_model))
print(tukey_test)
cat("\n")
}
set.seed(100)
options(warn=-1)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 10,
verbose = FALSE)
rfe_Profile <- rfe(x=MIRIS.HM.Data.new[, 2:7], y=MIRIS.HM.Data.new$Class,
sizes = c(1:8),
rfeControl = ctrl)
set.seed(100)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 10,
verbose = FALSE)
rfe_Profile <- rfe(x=MIRIS.HM.Data.new[, 2:7], y=MIRIS.HM.Data.new$Class,
sizes = c(1:8),
rfeControl = ctrl)
MIRIS.HM.Data.new$Class <-  factor(MIRIS.HM.Data.new$Class)
MIRIS.HM.Data.new$Class <-  factor(MIRIS.HM.Data.new$Class)
# Perform Leven's test for each continuous variable
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Levene's test
var_test <- leveneTest(formula, data = MIRIS.HM.Data.new)
# Print the variable name and the test results
cat("Variable:", names(MIRIS.HM.Data.new)[i], "\n")
print(var_test)
cat("\n")
}
# Perform ANOVA and display summary for each model
for (i in 2:7) {
# Create the formula
formula <- as.formula(paste(names(MIRIS.HM.Data.new)[i], "~ Class"))
# Perform Anova
aov_model <- aov(formula, data = MIRIS.HM.Data.new)
# Perform Tukey Test
tukey_test <- TukeyHSD(aov_model)
# Print Results
cat("Variable:", names(MIRIS.HM.Data.new)[i], "\n")
print(summary(aov_model))
print(tukey_test)
cat("\n")
}
set.seed(100)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 10,
verbose = FALSE)
rfe_Profile <- rfe(x=MIRIS.HM.Data.new[, 2:7], y=MIRIS.HM.Data.new$Class,
sizes = c(1:8),
rfeControl = ctrl)
rfe_Profile
# Split the data into training and test sets (80% training, 20% test)
Train_Index <- createDataPartition(
y = MIRIS.HM.Data.scale$TS,
## the outcome data are needed
p = .70,
## The percentage of data in the
## training set
list = FALSE
)
train_Data <- MIRIS.HM.Data[ Train_Index, -1]
test_Data  <- MIRIS.HM.Data[-Train_Index, -1]
fitControl <- trainControl(method = "repeatedcv",
number = 10,     # number of folds
repeats = 10)    # repeated ten times
# Train the model using Linear Regression and predict on the training data itself.
lm_model <-  train(TS ~ ., data=train_Data, method='lm', trControl = fitControl)
# predict on test data
lm_model_fitted <- predict(lm_model)
lm_model
# Train the model using Linear Regression and predict on the training data itself.
Svm_model <-  train(TS ~ ., data=train_Data, method='svmRadial', trControl = fitControl)
# predict on test data
SVM_model_fitted <- predict(Svm_model)
Svm_model
# Train the model using Linear Regression and predict on the training data itself.
KNN_model <-  train(TS ~ ., data=train_Data, method='knn', trControl = fitControl)
# predict on test data
KNN_model_fitted <- predict(KNN_model)
KNN_model
# Train the model using Linear Regression and predict on the training data itself.
PLS_model <-  train(TS ~ ., data=train_Data, method='pls', trControl = fitControl, ncomp = 4)
# Train the model using Linear Regression and predict on the training data itself.
PLS_model <-  train(TS ~ ., data=train_Data, method='pls', trControl = fitControl, tuneGrid = expand.grid(ncomp = 4)))
# Train the model using Linear Regression and predict on the training data itself.
PLS_model <-  train(TS ~ ., data=train_Data, method='pls', trControl = fitControl, tuneGrid = expand.grid(ncomp = 4))
# predict on train model
PLS_model_fitted <- predict(PLS_model)
PLS_model
# Train the model using Linear Regression and predict on the training data itself.
PLS_model <-  train(TS ~ ., data=train_Data, method='pls', trControl = fitControl, tuneGrid = expand.grid(ncomp = 2))
# predict on train model
PLS_model_fitted <- predict(PLS_model)
PLS_model
# Train the model using Linear Regression and predict on the training data itself.
PLS_model <-  train(TS ~ ., data=train_Data, method='pls', trControl = fitControl, tuneGrid = expand.grid(ncomp = 2))
# predict on train model
PLS_model_fitted <- predict(PLS_model,test_Data)
PLS_model
View(PLS_model)
PLS_model[["metric"]]
View(MIRIS.HM.Data)
View(MIRIS.HM.Data.new)
# using caret lib to preprocess data
Normalization <- preProcess(MIRIS.HM.Data, method = c("center", "scale"),na.remove = TRUE )
# standardize the preprocessed data
MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
# Assigning Class to Variable  according to information from sample ID
MIRIS.HM.Data.new <- MIRIS.HM.Data.scale %>%
mutate(Class = case_when(grepl("^POST", Samples) ~ "post",
grepl("^PRE", Samples) ~ "pre",
grepl("^P", Samples) ~ "PI",
grepl("^T", Samples) ~ "TI",
TRUE ~ "other"
))
attach(MIRIS.HM.Data)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value",-Class, -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.new)
print(Milk.Boxplot)
# Calculate p-values for each correlation coefficient
p.mat <- cor_pmat(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Calculate correlation matrix using Pearson correlation
correlationMatrix <- cor(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Visualize the correlation matrix using ggcorrplot
ggcorrplot(
correlationMatrix,
hc.order = TRUE,   # Hierarchical clustering for reordering variables
type = "lower",    # Show only the lower triangle of the correlation matrix
lab = TRUE,        # Show labels for variables
p.mat = p.mat      # Overlay p-values on the plot
)
# Calculate covariance matrix
cov_matrix <- cov(MIRIS.HM.Data.scale[2:7], method = "pearson")
cov_matrix <- cov2cor(cov_matrix)
cov_df_long <- melt(cov_matrix)
# Create the covariance plot using ggplot2
plot <- ggplot(cov_df_long, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient(high = "red", low = "white") +
geom_text(aes(label = paste0(round(value * 100, 2))), color = "black", size = 3) +
labs(title = "Covariance Heatmap",
x = "Variable 1",
y = "Variable 2")
print(plot)
MIRIS.HM.PCA <- prcomp(MIRIS.HM.Data.new[2:7], scale = TRUE)
#Visualization of PCA
fviz_pca_ind(MIRIS.HM.PCA,
geom = "point",
habillage = MIRIS.HM.Data.new$Class,
palette = c("blue", "red","green","yellow"),
addEllipses = TRUE,
ellipse.type="confidence",
ggtheme = theme_bw(),
title = "PCA plot for HM Fatty Acids")
#Scree plot
fviz_eig(MIRIS.HM.PCA,
addlabels = TRUE,
ylim = c(0, 70),
main="Scree Plot Fatty Acids")
# Variable plot
fviz_pca_var(MIRIS.HM.PCA, col.var = "red")
# using caret lib to preprocess data
Normalization <- preProcess(MIRIS.HM.Data, method = c("range"),na.remove = TRUE )
# standardize the preprocessed data
MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
# Assigning Class to Variable  according to information from sample ID
MIRIS.HM.Data.new <- MIRIS.HM.Data.scale %>%
mutate(Class = case_when(grepl("^POST", Samples) ~ "post",
grepl("^PRE", Samples) ~ "pre",
grepl("^P", Samples) ~ "PI",
grepl("^T", Samples) ~ "TI",
TRUE ~ "other"
))
attach(MIRIS.HM.Data)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value",-Class, -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.new)
print(Milk.Boxplot)
# using caret lib to preprocess data
Normalization <- preProcess(MIRIS.HM.Data, method = c("YeoJohnson"),na.remove = TRUE )
# standardize the preprocessed data
MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
# Assigning Class to Variable  according to information from sample ID
MIRIS.HM.Data.new <- MIRIS.HM.Data.scale %>%
mutate(Class = case_when(grepl("^POST", Samples) ~ "post",
grepl("^PRE", Samples) ~ "pre",
grepl("^P", Samples) ~ "PI",
grepl("^T", Samples) ~ "TI",
TRUE ~ "other"
))
attach(MIRIS.HM.Data)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value",-Class, -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.new)
print(Milk.Boxplot)
MIRIS.HM.Data.scale <- cbind(MIRIS.HM.Data[1], log10(MIRIS.HM.Data[2:7]))
View(MIRIS.HM.Data.scale)
# using caret lib to preprocess data
#Normalization <- preProcess(MIRIS.HM.Data, method = c("center", "scale"),na.remove = TRUE )
# standardize the preprocessed data
#MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
MIRIS.HM.Data.scale <- cbind(MIRIS.HM.Data[1], log10(MIRIS.HM.Data[2:7]))
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
# using caret lib to preprocess data
#Normalization <- preProcess(MIRIS.HM.Data, method = c("center", "scale"),na.remove = TRUE )
# standardize the preprocessed data
#MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
MIRIS.HM.Data.scale <- cbind(MIRIS.HM.Data[1], log10(MIRIS.HM.Data[2:7]))
# Replacing the value Infinite value  with 0
for (col in names(MIRIS.HM.Data[2:7])) {
MIRIS.HM.Data[[col]][!is.finite(MIRIS.HM.Data[[col]])] <- 0
}
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
for (col in names(MIRIS.HM.Data[2:7])) {
MIRIS.HM.Data[[col]][!is.finite(MIRIS.HM.Data[[col]])] <- 0
}
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# using caret lib to preprocess data
Normalization <- preProcess(MIRIS.HM.Data, method = c("center", "scale"),na.remove = TRUE )
# standardize the preprocessed data
MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
# using caret lib to preprocess data
Normalization <- preProcess(MIRIS.HM.Data, method = c("BoxCox"),na.remove = TRUE )
# standardize the preprocessed data
MIRIS.HM.Data.scale <- predict(Normalization,MIRIS.HM.Data)
# Skewness Score After Normalization
skewness(MIRIS.HM.Data.scale[2:7])
# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS.HM.Data.scale)
skimmed_MIRIS_scale
# Assigning Class to Variable  according to information from sample ID
MIRIS.HM.Data.new <- MIRIS.HM.Data.scale %>%
mutate(Class = case_when(grepl("^POST", Samples) ~ "post",
grepl("^PRE", Samples) ~ "pre",
grepl("^P", Samples) ~ "PI",
grepl("^T", Samples) ~ "TI",
TRUE ~ "other"
))
attach(MIRIS.HM.Data)
# Boxplot Function
boxplot <- function(data){
data %>%
gather(key = "feature", value = "value",-Class, -Samples) %>%
ggplot(aes(x = Class, y = value, fill = Class)) +
geom_boxplot() +
facet_wrap(~ feature, scales = "free") +
labs(x = "Group", y = "Value")
}
Milk.Boxplot <- boxplot(MIRIS.HM.Data.new)
print(Milk.Boxplot)
# Calculate p-values for each correlation coefficient
p.mat <- cor_pmat(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Calculate correlation matrix using Pearson correlation
correlationMatrix <- cor(MIRIS.HM.Data.scale[2:7], method = "pearson")
# Visualize the correlation matrix using ggcorrplot
ggcorrplot(
correlationMatrix,
hc.order = TRUE,   # Hierarchical clustering for reordering variables
type = "lower",    # Show only the lower triangle of the correlation matrix
lab = TRUE,        # Show labels for variables
p.mat = p.mat      # Overlay p-values on the plot
)
# Calculate covariance matrix
cov_matrix <- cov(MIRIS.HM.Data.scale[2:7], method = "pearson")
cov_matrix <- cov2cor(cov_matrix)
cov_df_long <- melt(cov_matrix)
# Create the covariance plot using ggplot2
plot <- ggplot(cov_df_long, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_gradient(high = "red", low = "white") +
geom_text(aes(label = paste0(round(value * 100, 2))), color = "black", size = 3) +
labs(title = "Covariance Heatmap",
x = "Variable 1",
y = "Variable 2")
print(plot)
# Train the model using PLS and predict on the Test data.
RF_model <-  train(TS ~ ., data=train_Data, method='rf', trControl = fitControl)
# predict on train model
RF_model_fitted <- predict(RF_model,test_Data)
RF_model
