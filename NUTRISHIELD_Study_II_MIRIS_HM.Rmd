---
title: "NUTRISHIELD_Study_II_MIRIS_HM_EDA"
author: "Parth Doshi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = "png",
                      dpi = 300,
                      echo = TRUE,
                      cache = FALSE)
```

## Exploratory Data Analysis for NUTRISHIELD Study II MIRIS Human Milk Sample

## Required library

```{r message=FALSE, warning=FALSE}
# install required library
library(devtools)
#devtools::install_github("sfirke/janitor")
#install.packages("DescTools")
#install.packages("skimr")

# load Required libraries
library(tidyverse) # meta package of all tidyverse packages
library(janitor) # 
library(ggplot2)
library(corrplot)
library(ggcorrplot)
library(caret)
library(FactoMineR)
library(ggfortify)
library(factoextra)
library(moments)
library(reshape2)
library(ggbiplot)
library(skimr)
library(car)
```

## Data importing

```{r}
# Set Working Directory
setwd("C:/Users/Parth Doshi/Dropbox/Nutrishield_Study_II_Project (ParthD thesis)/R-script/EDA_for_Human_Mother_Milk")

#load Data
MIRIS_HM_Data <- read.csv("C:/Users/Parth Doshi/Dropbox/Nutrishield_Study_II_Project (ParthD thesis)/NSII_Corrected_and_Clean_Data/NSII_MIRIS_HM.csv",sep = ",")
str(MIRIS_HM_Data)

MIRIS_HM_Data <- as.data.frame(MIRIS_HM_Data)

# Select columns 2 to 13 and convert them to numeric using lapply
MIRIS_HM_Data[, 2:7] <- lapply(MIRIS_HM_Data[, 2:7], as.numeric)
  

# assigning new names to the columns of the data frame
write.csv(MIRIS_HM_Data,"C:/Users/Parth Doshi/Dropbox/Nutrishield_Study_II_Project (ParthD thesis)/Study2-clean-Data/NUTRISHIELD_Study_II_MIRIS_HM.csv")
```

```{r}
skimmed_MIRIS <- skim(MIRIS_HM_Data)
skimmed_MIRIS
```

## Data Prepossessing

```{r warning=FALSE}

# using caret lib to preprocess data
Normalization <- preProcess(MIRIS_HM_Data, method = "BoxCox",na.remove = TRUE )

# Normalization the preprocessed data
MIRIS_HM_Data_normal <- predict(Normalization,MIRIS_HM_Data)

# using caret lib to preprocess data
standardise <- preProcess(MIRIS_HM_Data_normal, method = c("center","scale"),na.remove = TRUE )

# Normalization the preprocessed data
MIRIS_HM_Data_scale <- predict(Normalization,MIRIS_HM_Data_normal)
MIRIS_HM_Data_scale[is.na(MIRIS_HM_Data_scale)] <- 0

# Summary Stats after Normalization
skimmed_MIRIS_scale <- skim_to_list(MIRIS_HM_Data_scale)
skimmed_MIRIS_scale
```

## Comparative anaylsis 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Boxplot Function 
boxplot <- function(data){
data %>%
    gather(key = "feature", value = "value", -Samples, -NewClass) %>% 
    ggplot(aes(x = NewClass, y = value, fill = NewClass)) +
    geom_boxplot() +
    facet_wrap(~ feature, scales = "free") +
    labs(x = "Group", y = "Value")
}

Milk_Boxplot <- boxplot(MIRIS_HM_Data_scale)
print(Milk_Boxplot)
```

##Exploratory Data Analysis

## correlation 

```{r}
# Calculate p-values for each correlation coefficient
p_mat <- cor_pmat(MIRIS_HM_Data_scale[2:7], method = "pearson")

# Calculate correlation matrix using Pearson correlation
correlationMatrix <- cor(MIRIS_HM_Data_scale[2:7], method = "pearson")

# Visualize the correlation matrix using ggcorrplot
ggcorrplot(
  correlationMatrix,
  hc.order = TRUE,   # Hierarchical clustering for reordering variables
  type = "lower",    # Show only the lower triangle of the correlation matrix
  lab = TRUE,        # Show labels for variables
  p.mat = p_mat      # Overlay p-values on the plot
)
```
### Covariance 

```{r}
# Calculate covariance matrix
cov_matrix <- cov(MIRIS_HM_Data_scale[2:7], method = "pearson")

cov_matrix <- cov2cor(cov_matrix)
cov_df_long <- melt(cov_matrix)


# Create the covariance plot using ggplot2
plot <- ggplot(cov_df_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(high = "red", low = "white") +
  geom_text(aes(label = paste0(round(value * 100, 2))), color = "black", size = 3) +
  labs(title = "Covariance Heatmap",
       x = "Variable 1",
       y = "Variable 2")


print(plot)
```

# PCA 
```{r}

MIRIS_HM_PCA <- prcomp(MIRIS_HM_Data_scale[2:7], scale = TRUE, center = TRUE)

#Visualization of PCA
fviz_pca_ind(MIRIS_HM_PCA,
             geom = "point",
             habillage = MIRIS_HM_Data_scale$NewClass,
             palette = c("blue", "red","green","yellow"),
             addEllipses = TRUE,
             ellipse.type="confidence",
             ggtheme = theme_bw(),
             title = "PCA plot for HM Fatty Acids")
#Scree plot 
fviz_eig(MIRIS_HM_PCA, 
         addlabels = TRUE, 
         ylim = c(0, 70),
         main="Scree Plot Fatty Acids")

# Variable plot
fviz_pca_var(MIRIS_HM_PCA, col.var = "red")
# Contribution of each Variable
var <- get_pca_var(MIRIS_HM_PCA)
PC1<-fviz_contrib(MIRIS_HM_PCA, "var", axes=1, xtickslab.rt=90)
PC2<-fviz_contrib(MIRIS_HM_PCA, "var", axes=2, xtickslab.rt=90)
PC3<-fviz_contrib(MIRIS_HM_PCA, "var", axes=3, xtickslab.rt=90)
PC4<-fviz_contrib(MIRIS_HM_PCA, "var", axes=4, xtickslab.rt=90)
PC5<-fviz_contrib(MIRIS_HM_PCA, "var", axes=5, xtickslab.rt=90)

plot(PC1,main = "Variables percentage contribution of first Principal Components")
plot(PC2,main = "Variables percentage contribution of Second Principal Components")
plot(PC3,main = "Variables percentage contribution of Third Principal Components")
plot(PC4,main = "Variables percentage contribution of Fourth Principal Components")
plot(PC5,main = "Variables percentage contribution of Fifth Principal Components")
```

### stastical test to identfy is  significance difference between variables 

```{r}
library(rstatix)

# Define the Continuous Variables and Categorical Variable
continuous_vars <- colnames(MIRIS_HM_Data_scale[2:7])
categorical_var <- "NewClass"


# Initialize an empty data frame to store the results
shapiro_results <- data.frame(Variable = character(0), P_Value = numeric(0), Significant = character(0), stringsAsFactors = FALSE)

# Loop through the continuous variables
for (vars in continuous_vars) {
  data_vector <- MIRIS_HM_Data_scale[[vars]]
  shapiro_result <- shapiro.test(data_vector)
  
  # Append the results to the data frame
  shapiro_results <- rbind(shapiro_results, data.frame(Variable = vars, P_Value = shapiro_result$p.value, Significant = ifelse(shapiro_result$p.value < 0.05, "**", ".")))
}

# Print the results as a table
print(shapiro_results)
```

### Leven's Test

```{r}

# perform levene's test to see if all group has equal variance

# Update the levels of the New Class variable
MIRIS_HM_Data_scale$NewClass <- factor(MIRIS_HM_Data_scale$NewClass)


# Initialize an empty data frame to store the results
levene_results <- data.frame(Variable = character(0), F_Value = numeric(0), P_Value = numeric(0), stringsAsFactors = FALSE)

# Loop through the continuous variables
for (vars in continuous_vars) {
  formula <- as.formula(paste(vars, "~", categorical_var))
  var_test <- leveneTest(formula, data = MIRIS_HM_Data_scale)
  levene_results <- rbind(levene_results, data.frame(Variable = vars, F_Value = var_test$`F value`, P_Value = var_test$`Pr(>F)`, Significant = ifelse(var_test$`Pr(>F)` < 0.05, "**", "-")))
}

print(levene_results)

```

#### KrusKal-Wallis test 

```{r}
# Perform Kruskal-Wallis test for each variable
# Initialize an empty data frame to store the Kruskal-Wallis results
kruskal_results <- data.frame(Variable = character(0), H_Statistic = numeric(0), P_Value = numeric(0), stringsAsFactors = FALSE)

# Loop through the continuous variables
for (vars in continuous_vars) {
  formula <- as.formula(paste(vars, "~", categorical_var))
  kruskal_model <- kruskal.test(formula, data = MIRIS_HM_Data_scale)
  
  # Append the results to the data frame
  kruskal_results <- rbind(kruskal_results, data.frame(Variable = vars, H_Statistic = kruskal_model$statistic, P_Value = kruskal_model$p.value, Significant = ifelse(kruskal_model$p.value < 0.05, "**", "-")))
}

print(kruskal_results)

# Perform pairwise dunn test and display summary for each model
for (var in continuous_vars) {
  formula <- as.formula(paste(var, "~", categorical_var))
  pairwise_test <- dunn_test(formula, data = MIRIS_HM_Data_scale, p.adjust.method = "bonferroni")
  cat("Variable:", var)
  print(pairwise_test)
  cat("/n")
}
```

```{r message=FALSE, warning=FALSE}
library(ggstatsplot)
library(rlang)

StatBoxplot <- function(y) {
  y_sym <- sym(y)
  plot <- ggbetweenstats(
    data = MIRIS_HM_Data_scale,
    x = NewClass,
    y = !!y_sym,
    type = "nonparametric",
    plot.type = "box",
    pairwise.comparisons = TRUE,
    pairwise.display = "all",
    centrality.plotting = FALSE,
    bf.message = FALSE,
    ylab = y
  )
  
  return(plot)
}

StatBoxplot(y = "Fat")
StatBoxplot(y = "TS")
StatBoxplot(y = "Energy")
StatBoxplot(y = "Crude_Protein")
StatBoxplot(y = "Carbohydates")
StatBoxplot(y = "True_Protein")
```

### Feature selection 

```{r}
set.seed(100)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 10,
                   verbose = FALSE)

rfe_Profile <- rfe(x=MIRIS_HM_Data_scale[, 2:7], y=MIRIS_HM_Data_scale$NewClass,
                 sizes = c(1:8),
                 rfeControl = ctrl)

rfe_Profile
```
### Regression Model 

```{r}
# Split the data into training and test sets (80% training, 20% test)
Train_Index <- createDataPartition(
  y = MIRIS_HM_Data_scale$TS,
  ## the outcome data are needed
  p = .70,
  ## The percentage of data in the
  ## training set
  list = FALSE
)

train_Data <- MIRIS_HM_Data[ Train_Index, -1]
test_Data  <- MIRIS_HM_Data[-Train_Index, -1]
```

# linear regression model

```{r}
fitControl <- trainControl(method = "repeatedcv",   
                           number = 10,     # number of folds
                           repeats = 10)    # repeated ten times

# Train the model using Linear Regression and predict on the training data itself.
lm_model <-  train(TS ~ ., data=train_Data, method='lm', trControl = fitControl)

# predict on test data
lm_model_fitted <- predict(lm_model, test_Data)
lm_model
```
### SVM-radial

```{r}
# Train the model using Linear SVM and predict on the Test data.
Svm_model <-  train(TS ~ ., data = train_Data, method = 'svmRadial', trControl = fitControl)

# predict on test data
SVM_model_fitted <- predict(Svm_model, test_Data)
Svm_model
```

### KNN-Regression

```{r}
# Train the model using Knn and predict on the test data.
KNN_model <-  train(TS ~ ., data=train_Data, method='knn', trControl = fitControl)

# predict on test data
KNN_model_fitted <- predict(KNN_model, test_Data)
KNN_model
```

### PLS
```{r}
# Train the model using PLS and predict on the Test data.
PLS_model <-  train(TS ~ ., data=train_Data, method='pls', trControl = fitControl, tuneGrid = expand.grid(ncomp = 2))

# predict on train model
PLS_model_fitted <- predict(PLS_model,test_Data)

PLS_model
```

### Random Forest

```{r}
# Train the model using PLS and predict on the Test data.
RF_model <-  train(TS ~ ., data=train_Data, method='rf', trControl = fitControl)

# predict on train model
RF_model_fitted <- predict(RF_model,test_Data)

RF_model
```